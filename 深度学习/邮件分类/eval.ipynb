{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14982\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x0000018402902828>\n",
      "BATCH_SIZE=<absl.flags._flag.Flag object at 0x00000184026A9978>\n",
      "CHECKPOINT_DIR=<absl.flags._flag.Flag object at 0x00000184026A9940>\n",
      "EVAL_TRAIN=<absl.flags._flag.BooleanFlag object at 0x000001840286F160>\n",
      "F=<absl.flags._flag.Flag object at 0x000001847A4988D0>\n",
      "INPUT_LABEL_FILE=<absl.flags._flag.Flag object at 0x0000018400BC13C8>\n",
      "INPUT_TEXT_FILE=<absl.flags._flag.Flag object at 0x000001847A47A080>\n",
      "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x00000184029028D0>\n",
      "\n",
      "Cannot find a valid checkpoint file!\n",
      "Using checkpoint file : None\n",
      "Word2vec model file '..\\trained_word2vec.model' doesn't exist!\n",
      "Using word2vec model file : ..\\trained_word2vec.model\n",
      "Training params file '..\\training_params.pickle' is missing!\n",
      "Using training params file : ..\\training_params.pickle\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\training_params.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-efe27029f287>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;31m# Load params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_params_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[0mnum_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'num_labels'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[0mmax_document_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_document_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\深度学习\\邮件分类\\data_helpers.py\u001b[0m in \u001b[0;36mloadDict\u001b[1;34m(dict_file)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloadDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\training_params.pickle'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "import word2vec_helpers\n",
    "from text_cnn import TextCNN\n",
    "import csv\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data Parameters\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')# ??????????\n",
    "tf.flags.DEFINE_string(\"input_text_file\", \"./data/spam_100.utf8\", \"Test text data source to evaluate.\")\n",
    "tf.flags.DEFINE_string(\"input_label_file\", \"\", \"Label file for test text data source.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", True, \"Evaluate on all training data\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS.flag_values_dict()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "# validate\n",
    "# ==================================================\n",
    "\n",
    "# validate checkout point file\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "if checkpoint_file is None:\n",
    "    print(\"Cannot find a valid checkpoint file!\")\n",
    "    exit(0)\n",
    "print(\"Using checkpoint file : {}\".format(checkpoint_file))\n",
    "\n",
    "# validate word2vec model file\n",
    "trained_word2vec_model_file = os.path.join(FLAGS.checkpoint_dir, \"..\", \"trained_word2vec.model\")\n",
    "if not os.path.exists(trained_word2vec_model_file):\n",
    "    print(\"Word2vec model file \\'{}\\' doesn't exist!\".format(trained_word2vec_model_file))\n",
    "print(\"Using word2vec model file : {}\".format(trained_word2vec_model_file))\n",
    "\n",
    "# validate training params file\n",
    "training_params_file = os.path.join(FLAGS.checkpoint_dir, \"..\", \"training_params.pickle\")\n",
    "if not os.path.exists(training_params_file):\n",
    "    print(\"Training params file \\'{}\\' is missing!\".format(training_params_file))\n",
    "print(\"Using training params file : {}\".format(training_params_file))\n",
    "\n",
    "# Load params\n",
    "params = data_helpers.loadDict(training_params_file)\n",
    "num_labels = int(params['num_labels'])\n",
    "max_document_length = int(params['max_document_length'])\n",
    "\n",
    "# Load data\n",
    "if FLAGS.eval_train:\n",
    "    x_raw, y_test = data_helpers.load_data_and_labels(FLAGS.input_text_file, FLAGS.input_label_file, num_labels)\n",
    "else:\n",
    "    x_raw = [\"a masterpiece four years in the making\", \"everything is off.\"]\n",
    "    y_test = [1, 0]\n",
    "\n",
    "# Get Embedding vector x_test\n",
    "sentences, max_document_length = data_helpers.padding_sentences(x_raw, '<PADDING>', padding_sentence_length = max_document_length)\n",
    "x_test = np.array(word2vec_helpers.embedding_sentences(sentences, file_to_load = trained_word2vec_model_file))\n",
    "print(\"x_test.shape = {}\".format(x_test.shape))\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "checkpoint_file = tf.train.latest_checkpoint(FLAGS.checkpoint_dir)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array([text.encode('utf-8') for text in x_raw]), all_predictions))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \"..\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
